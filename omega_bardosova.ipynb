{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install ujson\n",
    "!pip install xnetwork\n",
    "!pip install infomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from os.path import join as PJ\n",
    "# import bgzf\n",
    "import struct\n",
    "import os\n",
    "import numpy as np\n",
    "import operator\n",
    "import gensim\n",
    "import ujson\n",
    "import igraph as ig\n",
    "import xnetwork as xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateinput = '15102022'\n",
    "dateoutput = '09112022'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import xnetwork as xn\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from os.path import join as PJ\n",
    "\n",
    "# from igraph import *\n",
    "# from mpmath import mp # pip install mpmath\n",
    "# from scipy import integrate\n",
    "# mp.dps = 50\n",
    "\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !infomap --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from infomap import Infomap\n",
    "def infomapMembership(vertexCount,edges):\n",
    "    im = Infomap(\"-N 10 --ftree --silent --seed %d\"%np.random.randint(4294967296));\n",
    "    im.setVerbosity(0);\n",
    "    for nodeIndex in range(0,vertexCount):\n",
    "        im.add_node(nodeIndex)\n",
    "    for edge in edges:\n",
    "        im.add_link(edge[0], edge[1]);\n",
    "    im.run()\n",
    "    # print(\"Result\")\n",
    "    # print(\"\\n#node module\")\n",
    "    membership = [0]*vertexCount;\n",
    "    for node in im.tree:\n",
    "        if node.is_leaf:\n",
    "            membership[node.node_id] = node.module_id;\n",
    "    return membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infomapApply(g, weights=None):\n",
    "    vertexCount = g.vcount()\n",
    "    if(weights):\n",
    "        edges = [(e.source, e.target, e[weights]) for e in g.es]\n",
    "    else:\n",
    "        edges = g.get_edgelist()\n",
    "\n",
    "#     if(g.is_directed()):\n",
    "#         extraOptions = \"-d\"\n",
    "#     else:\n",
    "    extraOptions = \"\"\n",
    "    im = Infomap(\"%s -N 10 --silent --seed %d\" %\n",
    "                 (extraOptions, np.random.randint(4294967296)))\n",
    "    \n",
    "    im.setVerbosity(0)\n",
    "    for nodeIndex in range(0, vertexCount):\n",
    "        im.add_node(nodeIndex)\n",
    "    for edge in edges:\n",
    "        if(len(edge) > 2):\n",
    "            if(edge[2]>0):\n",
    "                im.addLink(edge[0], edge[1], edge[2])\n",
    "            im.add_link(edge[0], edge[1], weight=edge[2])\n",
    "        else:\n",
    "            im.add_link(edge[0], edge[1])\n",
    "\n",
    "    im.run()\n",
    "    membership = [\":\".join([str(a) for a in membership])\n",
    "                  for index, membership in im.get_multilevel_modules().items()]\n",
    "\n",
    "    levelMembership = []\n",
    "    levelCount = max([len(element.split(\":\")) for element in membership])\n",
    "    for level in range(levelCount):\n",
    "        print(level)\n",
    "        levelMembership.append(\n",
    "            [\":\".join(element.split(\":\")[:(level+1)]) for element in membership]\n",
    "        )\n",
    "    return levelMembership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 3, 3, 3, 2, 1, 2, 3, 2, 2, 2, 1, 1, 3, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "g = ig.Graph.Famous('Zachary')\n",
    "g.ecount()\n",
    "edges = [e.tuple for e in g.es]\n",
    "membership = infomapMembership(g.vcount(), edges)\n",
    "print(membership)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading manual dictionary an ignore list.\n",
      "Setting up nltk environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/acmbrito/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/acmbrito/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords;\n",
    "from nltk.stem.wordnet import WordNetLemmatizer;\n",
    "import nltk.data;\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize;\n",
    "from nltk.corpus import wordnet;\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "verboseMode = True;\n",
    "\n",
    "# Loading manual dictionary an ignore list\n",
    "if(verboseMode): print(\"Loading manual dictionary an ignore list.\");\n",
    "replaceDictionary = {};\n",
    "# with open(\"replaceDictionary.dat\",\"r\") as fp:\n",
    "# \tfor line in fp:\n",
    "# \t\tentry = line.strip().split(\"\\t\");\n",
    "# \t\tif(len(entry)>1):\n",
    "# \t\t\treplaceDictionary[entry[0]] = entry[1];\n",
    "\n",
    "# ignoreSet = set();\n",
    "# with open(\"ignoreSet.dat\",\"r\") as fp:\n",
    "# \tfor line in fp:\n",
    "# \t\tignoreSet.add(line.strip());\n",
    "\n",
    "\n",
    "#Setting up nltk environment\n",
    "if(verboseMode): print(\"Setting up nltk environment.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/acmbrito/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/acmbrito/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yourself', 'until', 'against', 'again', 'we', 'because', 'were', 'why', 'as', 'own', 'ain', \"mustn't\", 'which', 'wouldn', 'hers', 'into', 'nor', 'between', 'mustn', 'more', \"hasn't\", 'who', 'under', 'through', 'an', 'or', \"you're\", 'itself', 'to', 'hasn', 'both', 'before', 'over', 'only', \"won't\", 'too', 'not', \"you'll\", 's', \"hadn't\", 'during', 'be', 'her', 'she', 'for', 'while', 'off', 'whom', 'very', 'how', 'is', 'so', 'was', 'd', 'few', 'if', 'but', 'there', \"doesn't\", 'haven', \"aren't\", 'than', 'no', 'once', 'yourselves', 'further', 'of', 'being', 'o', 'up', \"wasn't\", \"needn't\", 'will', 'y', 'below', \"you'd\", 'these', \"mightn't\", 'shan', \"she's\", 'about', 'theirs', 'ourselves', 'all', 'those', \"didn't\", \"wouldn't\", 'did', 'doing', 'then', 'that', 'had', 'themselves', 't', 'shouldn', 'what', \"shan't\", 'our', 'don', 'each', 'won', 'herself', 'am', \"couldn't\", 'the', 'mightn', 'ma', 'his', 'this', \"should've\", 'their', 'weren', 'where', 'hadn', 'are', 'you', 'myself', 'they', \"don't\", 'aren', \"it's\", 'having', 'such', 'other', 'wasn', 'do', \"you've\", 'isn', 'him', 'its', 'when', 'them', 'have', 'out', \"isn't\", 'yours', 'it', 'any', 'same', 'm', 'he', 'should', 'on', 're', 'down', 'needn', 'above', 'with', 'here', 'most', 'your', 'at', \"shouldn't\", 'some', 'didn', 'and', 'from', 'himself', 'i', 'does', 'in', 'now', \"haven't\", 'after', \"that'll\", 'ours', 'been', 'couldn', 'by', 'just', 'can', 'me', 'a', 'doesn', 'has', 'll', 've', 'my', \"weren't\"}\n",
      "Loading manual dictionary an ignore list.\n",
      "Setting up nltk environment.\n",
      "Setting up tokenizer.\n"
     ]
    }
   ],
   "source": [
    "%%cython\n",
    "from nltk.corpus import stopwords;\n",
    "from nltk.stem.wordnet import WordNetLemmatizer;\n",
    "import nltk.data;\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize;\n",
    "from nltk.corpus import wordnet;\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "lmtzr = WordNetLemmatizer();\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "punctuation = re.compile(r'[(\\])(\\})(\\{)(\\[).?!,\":;()|]');\n",
    "stopSet = set(stopwords.words('english'));\n",
    "\n",
    "print(stopSet)\n",
    "\n",
    "verboseMode = True;\n",
    "\n",
    "# Loading manual dictionary an ignore list\n",
    "if(verboseMode): print(\"Loading manual dictionary an ignore list.\");\n",
    "replaceDictionary = {};\n",
    "with open(\"replaceDictionary.txt\",\"r\") as fp:\n",
    "\tfor line in fp:\n",
    "\t\tentry = line.strip().split(\"\\t\");\n",
    "\t\tif(len(entry)>1):\n",
    "\t\t\treplaceDictionary[entry[0]] = entry[1];\n",
    "\n",
    "ignoreSet = set();\n",
    "with open(\"ignoreSet.txt\",\"r\") as fp:\n",
    "\tfor line in fp:\n",
    "\t\tignoreSet.add(line.strip());\n",
    "\n",
    "\n",
    "#Setting up nltk environment\n",
    "if(verboseMode): print(\"Setting up nltk environment.\");\n",
    "\n",
    "\n",
    "def findWholeWord(w):\n",
    "\treturn re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\tif treebank_tag.startswith('J'):\n",
    "\t\treturn wordnet.ADJ\n",
    "\telif treebank_tag.startswith('V'):\n",
    "\t\treturn wordnet.VERB\n",
    "\telif treebank_tag.startswith('N'):\n",
    "\t\treturn wordnet.NOUN\n",
    "\telif treebank_tag.startswith('R'):\n",
    "\t\treturn wordnet.ADV\n",
    "\telse:\n",
    "\t\treturn ''\n",
    "\n",
    "# def get_wordnet_pos(treebank_tag):\n",
    "# \tif treebank_tag.startswith('J'):\n",
    "# \t\treturn -1\n",
    "# \telif treebank_tag.startswith('V'):\n",
    "# \t\treturn -1\n",
    "# \telif treebank_tag.startswith('N'):\n",
    "# \t\treturn wordnet.NOUN\n",
    "# \telif treebank_tag.startswith('R'):\n",
    "# \t\treturn wordnet.ADV\n",
    "# \telse:\n",
    "# \t\treturn ''\n",
    "\n",
    "#Setting up tokenizer\n",
    "if(verboseMode): print(\"Setting up tokenizer.\");\n",
    "\n",
    "tokenizerInput = {\n",
    "\t\"stopSet\":stopSet,\n",
    "\t\"punctuation\":punctuation,\n",
    "\t\"tokenizer\":sent_tokenizer,\n",
    "\t\"lematizer\":lmtzr,\n",
    "\t\"sent_tokenize\": sent_tokenize,\n",
    "\t\"replaceDictionary\": replaceDictionary,\n",
    "\t\"ignoreSet\":ignoreSet\n",
    "}\n",
    "\n",
    "def tokenizeString(theString,maximumTokenSize,tokenizerInput,removeStopWords=True):\n",
    "\tstopSet = tokenizerInput[\"stopSet\"];\n",
    "\tlematizer = tokenizerInput[\"lematizer\"];\n",
    "\ttokenizer = tokenizerInput[\"tokenizer\"];\n",
    "\tpunctuation = tokenizerInput[\"punctuation\"];\n",
    "\tsent_tokenize = tokenizerInput[\"sent_tokenize\"];\n",
    "\treplaceDictionary = tokenizerInput[\"replaceDictionary\"];\n",
    "\tignoreSet = tokenizerInput[\"ignoreSet\"];\n",
    "\twordsList = [];\n",
    "\ttitleAbstract = (\". \".join(theString.split(\"::\"))).strip();\n",
    "\twordsSentences = [word_tokenize(t) for t in sent_tokenize(titleAbstract)];\n",
    "\tstopSentence = False;\n",
    "\tfor si, words in enumerate(wordsSentences):\n",
    "\t\twordsTags = nltk.pos_tag(words);\n",
    "\t\tif(stopSentence):\n",
    "\t\t\tbreak;\n",
    "\t\tfor wi,wordTag in enumerate(wordsTags):\n",
    "\t\t\tword = wordTag[0];\n",
    "\t\t\ttag = wordTag[1];\n",
    "\t\t\t\n",
    "\t\t\tif word.isdigit() or word[1:].isdigit():\n",
    "\t\t\t\tcontinue;\n",
    "# \t\t\tif(si>len(wordsSentences)-4 and (word.lower()==\"copyright\" or (wi>0 and word.lower()==\"c\" and words[wi-1] == \"(\"  and words[wi+1] == \")\" ))):\n",
    "# \t\t\t\tstopSentence = True;\n",
    "# \t\t\t\tbreak;\n",
    "\t\t\tword = punctuation.sub(\"\", word);\n",
    "\t\t\tconvTag = get_wordnet_pos(tag);\n",
    "\t\t\t#print \"w: \"+word;\n",
    "\t\t\tif convTag == -1:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif(convTag != ''):\n",
    "\t\t\t\tword  = lematizer.lemmatize(word.lower(), convTag);\n",
    "\t\t\telse:\n",
    "\t\t\t\tword  = lematizer.lemmatize(word.lower());\n",
    "\t\t\tif(len(word)==0 or ((word in stopSet) and removeStopWords) or (word in ignoreSet)):\n",
    "\t\t\t\tcontinue;\n",
    "\t\t\telse:\n",
    "\t\t\t\tif(word in replaceDictionary):\n",
    "\t\t\t\t\twordsList.append(replaceDictionary[word]);\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\twordsList.append(word);\n",
    "\n",
    "\ttokens = [set() for i in range(maximumTokenSize)];\n",
    "\tfor wordIndex in range(len(wordsList)):\n",
    "\t\tfor tokenSize in range(0,min(wordIndex+1,maximumTokenSize)):\n",
    "\t\t\ttokens[tokenSize].add(\" \".join(wordsList[(wordIndex-tokenSize):(wordIndex+1)]));\n",
    "\treturn tokens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset_chu/citation_net_acs appl. mater. interfaces_011122.xnet\n",
      "subset_chu/citation_net_acs nano_011122.xnet\n",
      "subset_chu/citation_net_adv. funct. mater._011122.xnet\n",
      "subset_chu/citation_net_adv. mater._011122.xnet\n",
      "subset_chu/citation_net_chem. mat._011122.xnet\n",
      "subset_chu/citation_net_j. am. chem. soc._011122.xnet\n",
      "subset_chu/citation_net_j. mat. chem. b_011122.xnet\n",
      "subset_chu/citation_net_j. mater. chem. a_011122.xnet\n",
      "subset_chu/citation_net_j. mater. chem. c_011122.xnet\n",
      "subset_chu/citation_net_j. mater. chem._011122.xnet\n",
      "subset_chu/citation_net_langmuir_011122.xnet\n",
      "subset_chu/citation_net_macromolecules_011122.xnet\n",
      "subset_chu/citation_net_nano lett._011122.xnet\n",
      "subset_chu/citation_net_nat. mater._011122.xnet\n",
      "subset_chu/citation_net_nat. nanotechnol._011122.xnet\n"
     ]
    }
   ],
   "source": [
    "# couplingNetwork.vs[\"Year\"] = CI2Year Talvez\n",
    "import glob\n",
    "# files = glob.glob('subset_chu/citation_net_*180122.xnet')\n",
    "# files = glob.glob('subset_chu/citation_net_*%s.xnet' % dateinput)\n",
    "files = glob.glob('subset_chu/citation_net_*011122.xnet')\n",
    "for f in files:\n",
    "    print(f)\n",
    "files = ['subset_chu/citation_net_11journal_09112022.xnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining the major connected component.\n",
      "Tokenizing the abstracts.\n",
      "Done                   26             \n",
      "Obtaining the network community structure.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "<class 'int'> 15 1 132926\n",
      "\n",
      "Getting tokens frequency.\n",
      "Calculating the importance Index.\n",
      "A - nanoparticles, delivery, therapy, cancer, drug, imaging, tumor, release, protein, cell\n",
      "B - battery, carbon, anode, lithium, cathode, storage, oxygen, reaction, electrode, lithium-ion\n",
      "C - carbon nanotube, flexible, sensor, composite, membrane, hydrogel, transparent, graphene, film, liquid\n",
      "D - polymer, perovskite solar cell, transistor, efficient, efficiency, organic solar cell, layer, light-emitting diode, base, device\n",
      "E - graphene, thermoelectric, mos2, monolayer, two-dimensional, 2d, van der waals, heterostructures, transistor, boron\n",
      "F - photocatalytic, dye-sensitized solar cell, water, tio2, photoelectrochemical, efficient, enhanced, hydrogen, activity, light\n",
      "G - plasmonic, gold, optical, plasmon, sers, surface-enhanced raman, resonance, array, detection, nanoparticles\n",
      "H - metal-organic framework, membrane, co2, separation, porous, adsorption, capture, polymer, organic framework, selective\n",
      "I - nanocrystals, solar cell, nanocrystal, cdse, synthesis, pb, colloidal quantum dot, semiconductor, electron, ligand\n",
      "J - ferroelectric, magnetic, thin film, dielectric, ceramic, domain, effect, epitaxial, energy, interface\n",
      "K - nanowires, nanowire, silicon, quantum, growth, si, single, gaas, gan, array\n",
      "L - molecular, junction, molecule, spin, single-molecule, single, surface, transport, conductance, magnetic\n",
      "M - oxygen, oxide fuel cell, solid oxide fuel, cathode, perovskite, proton, conductivity, temperature, electrolyte, anode\n",
      "N - atomic layer deposition, thin film, using, precursor, growth, thermal, molecular layer, reaction, etching, area-selective\n",
      "O - metallic glass, nucleation, structural, ultrastable, polymer glass, nanoglasses, nanoglass, vapour deposition, two-step, homogeneous\n",
      "P - \n",
      "Saving the network.\n"
     ]
    }
   ],
   "source": [
    "# Some util functions\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    jsonFileprefix = file[:-5] + '_bardo'\n",
    "    removeStopWords = True;\n",
    "    maximumTokenSize = 3; #n-gram\n",
    "    minKeywordsPerCluster = 10;\n",
    "    maxKeywordsPerCluster = 10;\n",
    "    maxClusterNameLength = 150;\n",
    "    useMajorComponent = True;\n",
    "    verboseMode = True;\n",
    "\n",
    "    network = xn.xnet2igraph(file)\n",
    "    network.vs['wos_id'] = network.vs['name']\n",
    "    network.vs['name'] = network.vs['title']\n",
    "    \n",
    "    \n",
    "    # Obtaining the major connected component (if needed)\n",
    "    if(useMajorComponent):\n",
    "        if(verboseMode): print(\"Obtaining the major connected component.\");\n",
    "        network = network.clusters(\"WEAK\").giant();\n",
    "\n",
    "    # Tokenizing the abstracts\n",
    "    if(verboseMode): print(\"Tokenizing the abstracts.\");\n",
    "    tokensFrequency = [[] for i in range(maximumTokenSize)];\n",
    "    tokensGroupFrequency = [{} for i in range(maximumTokenSize)];\n",
    "\n",
    "    propertiesKeys = set();\n",
    "\n",
    "    verticesTokens = [];\n",
    "    for vertexIndex in range(network.vcount()):\n",
    "        if(vertexIndex%100==0):\n",
    "            print(\"Tokenizing: %d/%d             \"%(vertexIndex,network.vcount()),end=\"\\r\")\n",
    "\n",
    "    #         for wordsList in tokenList:\n",
    "    #             tokens = [set() for i in range(maximumTokenSize)];\n",
    "    #             for wordIndex in range(len(wordsList)):\n",
    "    #                 for tokenSize in range(0,min(wordIndex+1,maximumTokenSize)):\n",
    "    #                     tokens[tokenSize].add(\" \".join(wordsList[(wordIndex-tokenSize):(wordIndex+1)]));\n",
    "        verticesTokens.append(tokenizeString(network.vs[vertexIndex][\"title\"],maximumTokenSize,tokenizerInput));\n",
    "\n",
    "    print(\"Done                   \");\n",
    "\n",
    "    # Obtaining the network community structure\n",
    "    if(verboseMode): print(\"Obtaining the network community structure.\");\n",
    "    \n",
    "\n",
    "    edgelist = [(e.source,e.target) for e in network.es]\n",
    "    communities = infomapApply(network)[0]\n",
    "    communities = [int(c) for c in communities]\n",
    "    print(type(communities[0]), max(communities), min(communities), len(communities))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # print(\"Modularity: %f\"%cc.q);\n",
    "\n",
    "    clusters = [[] for i in range(max(communities)+1)];\n",
    "    for vertexIndex in range(network.vcount()):\n",
    "        clusters[communities[vertexIndex]].append(vertexIndex);\n",
    "\n",
    "    #sorting the clusters by size\n",
    "    clusters = sorted(clusters,key=len,reverse=True);\n",
    "\n",
    "    # Getting tokens frequency\n",
    "    if(verboseMode): print(\"Getting tokens frequency.\");\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    tokenFrequencyInClusters = [];\n",
    "    tokenFrequencyInCorpus = {};\n",
    "\n",
    "    for clusterIndex in range(len(clusters)):\n",
    "        cluster = clusters[clusterIndex];\n",
    "        tokenFrequencyInCluster = {};\n",
    "        for vertexIndex in cluster:\n",
    "            tokens = verticesTokens[vertexIndex];\n",
    "            for tokenSize in range(0,maximumTokenSize):\n",
    "                for token in tokens[tokenSize]:\n",
    "                    if(token not in tokenFrequencyInCorpus):\n",
    "                        tokenFrequencyInCorpus[token] = 0;\n",
    "                    if(token not in tokenFrequencyInCluster):\n",
    "                        tokenFrequencyInCluster[token] = 0;\n",
    "                    tokenFrequencyInCorpus[token] += 1;\n",
    "                    tokenFrequencyInCluster[token] += 1;\n",
    "        tokenFrequencyInClusters.append(tokenFrequencyInCluster);\n",
    "\n",
    "    # Calculating the importance Index\n",
    "    if(verboseMode): print(\"Calculating the importance Index.\");\n",
    "    #tokenRelativeFrequencyInClusters = [];\n",
    "    #tokenRelativeFrequencyOutClusters = [];\n",
    "    tokenImportanceIndexInClusters = [];\n",
    "\n",
    "    verticesCount = network.vcount();\n",
    "    for clusterIndex in range(len(clusters)):\n",
    "        clusterSize = len(clusters[clusterIndex]);\n",
    "\n",
    "        tokenFrequencyInCluster = tokenFrequencyInClusters[clusterIndex];\n",
    "\n",
    "        #tokenRelativeFrequencyInCluster = {};\n",
    "        #tokenRelativeFrequencyOutCluster = {};\n",
    "        tokenImportanceIndexInCluster = {};\n",
    "\n",
    "        for token in tokenFrequencyInCluster:\n",
    "            nInCluster = tokenFrequencyInCluster[token];\n",
    "            nOutCluster = tokenFrequencyInCorpus[token]-nInCluster;\n",
    "            outClusterSize = verticesCount-clusterSize;\n",
    "            if(nOutCluster==0):\n",
    "                outClusterSize = 1; #Fix for singletons\n",
    "            FInCluster = float(nInCluster)/float(clusterSize);\n",
    "            FOutCluster = float(nOutCluster)/float(outClusterSize);\n",
    "            importanceIndex = FInCluster-FOutCluster;\n",
    "            #tokenRelativeFrequencyInCluster[token] = FInCluster;\n",
    "            #tokenRelativeFrequencyOutCluster[token] = FOutCluster;\n",
    "            tokenImportanceIndexInCluster[token] = importanceIndex;\n",
    "\n",
    "        #tokenRelativeFrequencyInClusters.append(tokenRelativeFrequencyInCluster);\n",
    "        #tokenRelativeFrequencyOutClusters.append(tokenRelativeFrequencyOutCluster);\n",
    "        tokenImportanceIndexInClusters.append(tokenImportanceIndexInCluster);\n",
    "\n",
    "    defaultNames = \"ABCDEFGHIJKLMNOPQRSTUWVXYZ\";\n",
    "    defaultNamesLength = len(defaultNames);\n",
    "\n",
    "    clusterKeywords = [];\n",
    "    minClusterSize = min([len(cluster) for cluster in clusters]);\n",
    "    maxClusterSize = max([len(cluster) for cluster in clusters]);\n",
    "    clusterNames = [];\n",
    "    for clusterIndex in range(len(clusters)):\n",
    "        cluster = clusters[clusterIndex];\n",
    "        clusterSize = len(cluster);\n",
    "        keywords = [v[0] for v in sorted(tokenImportanceIndexInClusters[clusterIndex].items(),key=operator.itemgetter(1),reverse=True)];\n",
    "        if(maxClusterSize>minClusterSize):\n",
    "            m = (maxKeywordsPerCluster-minKeywordsPerCluster)/float(maxClusterSize-minClusterSize);\n",
    "        else:\n",
    "            m=0;\n",
    "        keywordsCount = round(m*(clusterSize-minClusterSize)+minKeywordsPerCluster);\n",
    "        currentKeywords = [];\n",
    "        while(len(currentKeywords)<keywordsCount and len(keywords)>len(currentKeywords)):\n",
    "            currentKeywords = keywords[0:keywordsCount];\n",
    "            jointKeywords = \".\"+\".\".join(currentKeywords)+\".\";\n",
    "            toRemoveKeywords = [];\n",
    "            for keyword in currentKeywords:\n",
    "                if(jointKeywords.find(\" %s.\"%keyword)>=0):\n",
    "                    toRemoveKeywords.append(keyword);\n",
    "                elif(jointKeywords.find(\".%s \"%keyword)>=0):\n",
    "                    toRemoveKeywords.append(keyword);\n",
    "            for toRemoveKeyword in toRemoveKeywords:\n",
    "                keywords.remove(toRemoveKeyword);\n",
    "                currentKeywords.remove(toRemoveKeyword);\n",
    "        clusterKeywords.append(currentKeywords);\n",
    "        #print(currentKeywords);\n",
    "        clusterName = \"\";\n",
    "        if(clusterIndex<defaultNamesLength):\n",
    "            clusterName += defaultNames[clusterIndex];\n",
    "        else:\n",
    "            clusterName += \"{%d}\"%(clusterIndex);\n",
    "        clusterName += \" - \"+\", \".join(currentKeywords);\n",
    "        if(len(clusterName)>maxClusterNameLength):\n",
    "            clusterName = clusterName[0:maxClusterNameLength-1]+\"...\";\n",
    "        for vertexIndex in cluster:\n",
    "            network.vs[vertexIndex][\"Cluster Name\"] = clusterName;\n",
    "            network.vs[vertexIndex][\"Cluster Index\"] = clusterIndex;\n",
    "        clusterNames.append(clusterName);\n",
    "        print(clusterName);\n",
    "\n",
    "\n",
    "    # Saving the network\n",
    "    if(verboseMode): print(\"Saving the network.\");\n",
    "    # network.vs[\"kcore\"] = network.coreness()\n",
    "\n",
    "    xn.igraph2xnet(network,fileName=PJ('',\"%s_infomap_%s.xnet\"%(jsonFileprefix, dateoutput)),ignoredNodeAtts=[\"Text\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(communities[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1191680"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
