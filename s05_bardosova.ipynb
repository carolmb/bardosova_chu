{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install ujson\n",
    "!pip install xnetwork\n",
    "!pip install infomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from os.path import join as PJ\n",
    "# import bgzf\n",
    "import struct\n",
    "import numpy as np\n",
    "import operator\n",
    "import gensim\n",
    "import ujson\n",
    "import igraph as ig\n",
    "import xnetwork as xn\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from os.path import join as PJ\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from infomap import Infomap\n",
    "def infomapMembership(vertexCount,edges):\n",
    "    im = Infomap(\"-N 10 --ftree --silent --seed %d\"%np.random.randint(4294967296));\n",
    "    im.setVerbosity(0);\n",
    "    for nodeIndex in range(0,vertexCount):\n",
    "        im.add_node(nodeIndex)\n",
    "    for edge in edges:\n",
    "        im.add_link(edge[0], edge[1]);\n",
    "    im.run()\n",
    "    # print(\"Result\")\n",
    "    # print(\"\\n#node module\")\n",
    "    membership = [0]*vertexCount;\n",
    "    for node in im.tree:\n",
    "        if node.is_leaf:\n",
    "            membership[node.node_id] = node.module_id;\n",
    "    return membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infomapApply(g, weights=None):\n",
    "    vertexCount = g.vcount()\n",
    "    if(weights):\n",
    "        edges = [(e.source, e.target, e[weights]) for e in g.es]\n",
    "    else:\n",
    "        edges = g.get_edgelist()\n",
    "\n",
    "#     if(g.is_directed()):\n",
    "#         extraOptions = \"-d\"\n",
    "#     else:\n",
    "    extraOptions = \"\"\n",
    "    im = Infomap(\"%s -N 10 --silent --seed %d\" %\n",
    "                 (extraOptions, np.random.randint(4294967296)))\n",
    "    \n",
    "    im.setVerbosity(0)\n",
    "    for nodeIndex in range(0, vertexCount):\n",
    "        im.add_node(nodeIndex)\n",
    "    for edge in edges:\n",
    "        if(len(edge) > 2):\n",
    "            if(edge[2]>0):\n",
    "                im.addLink(edge[0], edge[1], edge[2])\n",
    "            im.add_link(edge[0], edge[1], weight=edge[2])\n",
    "        else:\n",
    "            im.add_link(edge[0], edge[1])\n",
    "\n",
    "    im.run()\n",
    "    membership = [\":\".join([str(a) for a in membership])\n",
    "                  for index, membership in im.get_multilevel_modules().items()]\n",
    "\n",
    "    levelMembership = []\n",
    "    levelCount = max([len(element.split(\":\")) for element in membership])\n",
    "    for level in range(levelCount):\n",
    "        print(level)\n",
    "        levelMembership.append(\n",
    "            [\":\".join(element.split(\":\")[:(level+1)]) for element in membership]\n",
    "        )\n",
    "    return levelMembership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading manual dictionary an ignore list.\n",
      "Setting up nltk environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/acmbrito/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/acmbrito/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords;\n",
    "from nltk.stem.wordnet import WordNetLemmatizer;\n",
    "import nltk.data;\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize;\n",
    "from nltk.corpus import wordnet;\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "verboseMode = True;\n",
    "\n",
    "# Loading manual dictionary an ignore list\n",
    "if(verboseMode): print(\"Loading manual dictionary an ignore list.\");\n",
    "replaceDictionary = {};\n",
    "# with open(\"replaceDictionary.dat\",\"r\") as fp:\n",
    "# \tfor line in fp:\n",
    "# \t\tentry = line.strip().split(\"\\t\");\n",
    "# \t\tif(len(entry)>1):\n",
    "# \t\t\treplaceDictionary[entry[0]] = entry[1];\n",
    "\n",
    "# ignoreSet = set();\n",
    "# with open(\"ignoreSet.dat\",\"r\") as fp:\n",
    "# \tfor line in fp:\n",
    "# \t\tignoreSet.add(line.strip());\n",
    "\n",
    "\n",
    "#Setting up nltk environment\n",
    "if(verboseMode): print(\"Setting up nltk environment.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/acmbrito/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/acmbrito/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'with', 'ours', \"aren't\", 'our', \"didn't\", 'ain', 'under', 'who', 'not', 'about', 'ma', 'y', 'herself', 'each', 'nor', 'didn', 'my', \"wouldn't\", 'once', \"you'd\", 'were', 'so', \"you're\", 'she', 'we', 'up', 'himself', 'down', 'into', 'it', 'yours', 'being', 'this', 'the', 'again', 'very', \"don't\", 'll', 're', 'just', 'them', 'aren', 'what', 'or', 'you', 'yourself', \"wasn't\", 'other', 'on', 'by', 'while', 'before', \"you've\", 'needn', 'mustn', 'own', \"weren't\", 'doesn', 'its', 'few', 'both', 'having', \"you'll\", 'here', 'then', 'm', \"that'll\", 'through', 'where', 'her', 'was', 'myself', 'such', 'above', 'now', 'shouldn', \"should've\", 'should', \"hadn't\", \"mightn't\", 'no', \"haven't\", 'why', 'as', 'don', 'which', \"hasn't\", 'until', 'further', 'how', 'him', 'me', 'all', 's', 'is', 'ourselves', 'couldn', 'been', \"isn't\", 'some', 'does', 'wasn', 'whom', 'but', 'will', 'hasn', \"shouldn't\", 've', 'theirs', \"won't\", 'at', 'a', 'can', 'yourselves', 'shan', 'mightn', 'these', 'when', 'from', 'an', 'than', 'your', 'of', 'during', 'hers', 'below', \"she's\", 'off', 'only', 'themselves', 'had', 'out', 'his', 'itself', \"needn't\", 'are', 'any', 'do', 'did', 'i', 'that', 'those', 'has', 'between', 'to', 'more', \"shan't\", 'and', 'most', 'haven', 'be', 'against', 'too', 'doing', 'if', 'have', 'after', 't', \"doesn't\", \"mustn't\", 'wouldn', 'over', 'isn', \"it's\", \"couldn't\", 'weren', 'because', 'there', 'for', 'o', 'he', 'd', 'same', 'they', 'am', 'in', 'hadn', 'their', 'won'}\n",
      "Loading manual dictionary an ignore list.\n",
      "Setting up nltk environment.\n",
      "Setting up tokenizer.\n"
     ]
    }
   ],
   "source": [
    "%%cython\n",
    "from nltk.corpus import stopwords;\n",
    "from nltk.stem.wordnet import WordNetLemmatizer;\n",
    "import nltk.data;\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize;\n",
    "from nltk.corpus import wordnet;\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "lmtzr = WordNetLemmatizer();\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "punctuation = re.compile(r'[(\\])(\\})(\\{)(\\[).?!,\":;()|]');\n",
    "stopSet = set(stopwords.words('english'));\n",
    "\n",
    "print(stopSet)\n",
    "\n",
    "verboseMode = True;\n",
    "\n",
    "# Loading manual dictionary an ignore list\n",
    "if(verboseMode): print(\"Loading manual dictionary an ignore list.\");\n",
    "replaceDictionary = {};\n",
    "with open(\"replaceDictionary.txt\",\"r\") as fp:\n",
    "\tfor line in fp:\n",
    "\t\tentry = line.strip().split(\"\\t\");\n",
    "\t\tif(len(entry)>1):\n",
    "\t\t\treplaceDictionary[entry[0]] = entry[1];\n",
    "\n",
    "ignoreSet = set();\n",
    "with open(\"ignoreSet.txt\",\"r\") as fp:\n",
    "\tfor line in fp:\n",
    "\t\tignoreSet.add(line.strip());\n",
    "\n",
    "\n",
    "#Setting up nltk environment\n",
    "if(verboseMode): print(\"Setting up nltk environment.\");\n",
    "\n",
    "\n",
    "def findWholeWord(w):\n",
    "\treturn re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\tif treebank_tag.startswith('J'):\n",
    "\t\treturn wordnet.ADJ\n",
    "\telif treebank_tag.startswith('V'):\n",
    "\t\treturn wordnet.VERB\n",
    "\telif treebank_tag.startswith('N'):\n",
    "\t\treturn wordnet.NOUN\n",
    "\telif treebank_tag.startswith('R'):\n",
    "\t\treturn wordnet.ADV\n",
    "\telse:\n",
    "\t\treturn ''\n",
    "\n",
    "# def get_wordnet_pos(treebank_tag):\n",
    "# \tif treebank_tag.startswith('J'):\n",
    "# \t\treturn -1\n",
    "# \telif treebank_tag.startswith('V'):\n",
    "# \t\treturn -1\n",
    "# \telif treebank_tag.startswith('N'):\n",
    "# \t\treturn wordnet.NOUN\n",
    "# \telif treebank_tag.startswith('R'):\n",
    "# \t\treturn wordnet.ADV\n",
    "# \telse:\n",
    "# \t\treturn ''\n",
    "\n",
    "#Setting up tokenizer\n",
    "if(verboseMode): print(\"Setting up tokenizer.\");\n",
    "\n",
    "tokenizerInput = {\n",
    "\t\"stopSet\":stopSet,\n",
    "\t\"punctuation\":punctuation,\n",
    "\t\"tokenizer\":sent_tokenizer,\n",
    "\t\"lematizer\":lmtzr,\n",
    "\t\"sent_tokenize\": sent_tokenize,\n",
    "\t\"replaceDictionary\": replaceDictionary,\n",
    "\t\"ignoreSet\":ignoreSet\n",
    "}\n",
    "\n",
    "def tokenizeString(theString,maximumTokenSize,tokenizerInput,removeStopWords=True):\n",
    "\tstopSet = tokenizerInput[\"stopSet\"];\n",
    "\tlematizer = tokenizerInput[\"lematizer\"];\n",
    "\ttokenizer = tokenizerInput[\"tokenizer\"];\n",
    "\tpunctuation = tokenizerInput[\"punctuation\"];\n",
    "\tsent_tokenize = tokenizerInput[\"sent_tokenize\"];\n",
    "\treplaceDictionary = tokenizerInput[\"replaceDictionary\"];\n",
    "\tignoreSet = tokenizerInput[\"ignoreSet\"];\n",
    "\twordsList = [];\n",
    "\ttitleAbstract = (\". \".join(theString.split(\"::\"))).strip();\n",
    "\twordsSentences = [word_tokenize(t) for t in sent_tokenize(titleAbstract)];\n",
    "\tstopSentence = False;\n",
    "\tfor si, words in enumerate(wordsSentences):\n",
    "\t\twordsTags = nltk.pos_tag(words);\n",
    "\t\tif(stopSentence):\n",
    "\t\t\tbreak;\n",
    "\t\tfor wi,wordTag in enumerate(wordsTags):\n",
    "\t\t\tword = wordTag[0];\n",
    "\t\t\ttag = wordTag[1];\n",
    "\t\t\t\n",
    "\t\t\tif word.isdigit() or word[1:].isdigit():\n",
    "\t\t\t\tcontinue;\n",
    "# \t\t\tif(si>len(wordsSentences)-4 and (word.lower()==\"copyright\" or (wi>0 and word.lower()==\"c\" and words[wi-1] == \"(\"  and words[wi+1] == \")\" ))):\n",
    "# \t\t\t\tstopSentence = True;\n",
    "# \t\t\t\tbreak;\n",
    "\t\t\tword = punctuation.sub(\"\", word);\n",
    "\t\t\tconvTag = get_wordnet_pos(tag);\n",
    "\t\t\t#print \"w: \"+word;\n",
    "\t\t\tif convTag == -1:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif(convTag != ''):\n",
    "\t\t\t\tword  = lematizer.lemmatize(word.lower(), convTag);\n",
    "\t\t\telse:\n",
    "\t\t\t\tword  = lematizer.lemmatize(word.lower());\n",
    "\t\t\tif(len(word)==0 or ((word in stopSet) and removeStopWords) or (word in ignoreSet)):\n",
    "\t\t\t\tcontinue;\n",
    "\t\t\telse:\n",
    "\t\t\t\tif(word in replaceDictionary):\n",
    "\t\t\t\t\twordsList.append(replaceDictionary[word]);\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\twordsList.append(word);\n",
    "\n",
    "\ttokens = [set() for i in range(maximumTokenSize)];\n",
    "\tfor wordIndex in range(len(wordsList)):\n",
    "\t\tfor tokenSize in range(0,min(wordIndex+1,maximumTokenSize)):\n",
    "\t\t\ttokens[tokenSize].add(\" \".join(wordsList[(wordIndex-tokenSize):(wordIndex+1)]));\n",
    "\treturn tokens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['j. mat. chem. b_completedata_170423_allbut.xnet', 'j. mater. chem. a_completedata_170423_allbut.xnet', 'nat. mater._completedata_170423_allbut.xnet']\n"
     ]
    }
   ],
   "source": [
    "# couplingNetwork.vs[\"Year\"] = CI2Year Talvez\n",
    "\n",
    "files = ['ACS Appl. Mater. Interfaces_completedata_110423.xnet', 'Adv. Funct. Mater._completedata_110423.xnet']\n",
    "\n",
    "files = glob.glob('*_170423_allbut.xnet')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining the major connected component.\n",
      "Tokenizing the abstracts.\n",
      "Done                              \n",
      "Obtaining the network community structure.\n",
      "0\n",
      "1\n",
      "2\n",
      "\n",
      "Getting tokens frequency.\n",
      "Calculating the importance Index.\n",
      "A - hydrogel, injectable, tissue engineering, scaffold, mechanical, poly, shape, tough, collagen, wound\n",
      "B - bone, scaffold, bioactive, regeneration, hydroxyapatite, tissue, glass, osteogenic, titanium, calcium\n",
      "C - nanoparticles, cancer, imaging, photodynamic therapy, drug, photothermal therapy, tumor, near-infrared, imaging-guided, framework\n",
      "D - drug delivery, cancer, micelle, prodrug, doxorubicin, anticancer, intracellular, nanoparticles, drug release, polymeric\n",
      "E - surface, antibacterial, coating, antifouling, film, bacteria, membrane, polymer, antimicrobial, blood\n",
      "F - carbon dot, quantum dot, imaging, synthesis, fluorescent, bioimaging, nanoclusters, cell, detection, fluorescence\n",
      "G - fluorescent probe, detection, living cell, imaging, near-infrared, ratiometric, two-photon, emission, aie, image\n",
      "H - gene delivery, dna, gene transfection, vector, cationic, delivery system, efficiency, supramolecular, peptide, endothelial cell\n",
      "I - differentiation, scaffold, mesenchymal stem cell, human, graphene, 3d, culture, peptide, tissue, neural\n",
      "J - mesoporous silica nanoparticles, drug delivery, release, gold, nitric oxide, intracellular, system, cancer, therapy, target\n",
      "K - electrochemical, detection, immobilization, enzyme, sensor, graphene, non-enzymatic glucose, sensitive, framework, biosensor\n",
      "L - mri contrast agent, magnetic resonance imaging, poly, iron oxide nanoparticles, synthesis, biocompatible, mr imaging, tumor target, hyperthermia,...\n",
      "M - nanotube, biodegradable, transdermal, corrosion, coating, delivery, magnesium alloy, implant, tio2, vascular\n",
      "N - magnetic, selective, imprint, enrichment, protein, glycoprotein, microspheres, molecularly, phosphopeptides, highly\n",
      "O - detection, photoelectrochemical, base, gold, electrochemical, sensitive, immunosensor, biosensor, label-free, highly\n",
      "P - amyloid, silicon, aggregation, peptide, therapy, nanoparticles, cancer, fluorescence, therapeutic, carbon\n",
      "Q - colorimetric detection, glucose, peroxidase, h2o2, sensing, peroxidase-like activity, intrinsic, phosphatase, enzyme, application\n",
      "R - silica, supramolecular hydrogel, drug delivery system, photodynamic therapy, nanoparticles, effect, iron oxide, contrast agent, nanoparticle, sin...\n",
      "S - nanoparticles, tumor, carrier, drug delivery, chemotherapy, effect, beta-glucan, persistent, antitumor, cancer\n",
      "T - therapy, cancer cell, imaging, theranostic, two-photon, porous silicon nanoparticles, micelle, nanodiamond, multidrug, targeted\n",
      "U - zinc, base, fluorescent, selective, turn-on, oxygen, zwitterionic, breast cancer, ion, treatment\n",
      "W - film, antibacterial, release, electrophoretic deposition, chitosan, porous, coating, efficient gene transfection, cell, polymer\n",
      "V - hepatocellular carcinoma, biomimetic, biodegradable, polyphosphazene, multifunctional, nitric oxide release, target drug, tio2, system, tumor\n",
      "X - therapy, material, cancer, apoptosis, combination, chemo-photothermal, carbon nanotube, evaluation, chitosan, carrier\n",
      "Y - silk, chitosan-silica hybrid, hybrid scaffold, structure, regeneration, bone, trimethoxysilane, fabrication chitosan-silica, scaffold 3-glycidoxy...\n",
      "Z - \n",
      "Saving the network.\n",
      "Obtaining the major connected component.\n",
      "Tokenizing the abstracts.\n",
      "Done                                \n",
      "Obtaining the network community structure.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "\n",
      "Getting tokens frequency.\n",
      "Calculating the importance Index.\n",
      "A - lithium-ion battery, storage, lithium ion battery, anode material, cathode material, electrolyte, li-ion, sodium, sodium-ion, high\n",
      "B - perovskite solar cell, efficient, efficiency, organic solar cell, dye-sensitized solar cell, polymer solar cell, acceptor, photovoltaic, layer, e...\n",
      "C - efficient, evolution reaction, oxygen reduction, hydrogen evolution, catalyst, electrocatalysts, water, oxygen evolution, carbon, electrocatalyst\n",
      "D - membrane, metal-organic framework, co2, adsorption, separation, removal, capture, selective, organic, gas\n",
      "E - electrode, carbon, supercapacitor, high-performance, flexible, energy, graphene, asymmetric supercapacitors, electrochemical, high\n",
      "F - photoelectrochemical, water splitting, enhanced, visible light, tio2, photocatalytic activity, oxidation, production, photocatalytic hydrogen, ph...\n",
      "G - superhydrophobic, separation, self-healing, surface, coating, thermal, robust, membrane, aerogels, oil\n",
      "H - reduction, catalyst, reaction, electrocatalytic, nanoparticles, oxidation, methanol, activity, synthesis, catalytic\n",
      "I - oxide fuel cell, solid oxide fuel, oxygen, cathode, perovskite, temperature, effect, electrolysis, proton, surface\n",
      "J - n-type, thermoelectric material, high thermoelectric, enhanced thermoelectric, enhancement, thermal, alloy, p-type, compound, conductivity\n",
      "K - dielectric, energy storage, absorption, ceramic, energy density, lead-free, electromagnetic, nanocomposites, microwave, composite\n",
      "L - anion exchange membrane, alkaline, membrane fuel cell, vanadium, redox flow battery, poly arylene ether, sulfonated, proton exchange membrane, co...\n",
      "M - hydrogen storage, dehydrogenation, catalyst, nanoparticles, hydrazine, hydrogen generation, formic acid, ammonia, borane, catalytic\n",
      "N - detection, gas sensor, temperature, sense, gas sensing, selective, no2, h2s, highly sensitive, base\n",
      "O - energetic material, salt, high, promising, ionic liquid, synthesis, energetic compound, explosive, derivative, hypergolic\n",
      "P - mxene, mxenes, two-dimensional, hydrogen evolution, black phosphorus, 2d, titanium carbide, application, reaction, transition metal\n",
      "Q - solar cell, cu2znsns4, se, thin film, efficiency, photovoltaic, cu2znsnse4, cu ga, effect, defect\n",
      "R - tio2, solar thermal, energy storage, nayf4, system, base, near-infrared, photon, polyoxometalate, derivative\n",
      "S - \n",
      "Saving the network.\n",
      "Obtaining the major connected component.\n",
      "Tokenizing the abstracts.\n",
      "Done                              \n",
      "Obtaining the network community structure.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "\n",
      "Getting tokens frequency.\n",
      "Calculating the importance Index.\n",
      "A - spin, magnetic, effect, ferroelectric, domain, oxide, superconductivity, spintronics, film, wall\n",
      "B - self-assembly, colloidal, nanoparticles, assembly, nanocrystals, nanoparticle, dna, synthesis, particle, shape\n",
      "C - carbon nanotube, monolayer, van der waals, mos2, epitaxial graphene, two-dimensional material, boron nitride, 2d, transition, moire\n",
      "D - metamaterials, plasmonic, nanoparticles, resonance, optical, nanowire, surface, metal, laser, negative\n",
      "E - polymer, organic semiconductor, transistor, solar cell, molecular, efficiency, electronics, field-effect, transport, solution-processed\n",
      "F - stem cell, hydrogel, matrix, fate, differentiation, migration, mechanosensing, adhesion, regulate, biomaterials\n",
      "G - bone, deformation, composite, silk, nanocomposites, nucleation, crystal, spider, nanoparticle, bioinspired\n",
      "H - delivery, vaccine, polymer, cancer, tumour, protein, material, targeted, gene, synthetic\n",
      "I - electrode, lithium, electrolyte, electrochemical, redox, lithium-ion battery, cathode, energy storage, hydrogen, ionic\n",
      "J - catalyst, water, hydrogen evolution, nanoparticles, oxidation, platinum, catalytic, surface, reaction, alloy\n",
      "K - metal-organic framework, molecular, porous, zeolite, membrane, separation, nanoporous, sieving, silica, gas\n",
      "L - phase-change material, memory, electrochemical, memristors, organic, ion, storage, resistive, mechanism, switch\n",
      "M - liquid crystal, three-dimensional, polymer, liquid-crystal, block copolymer, photonic crystal, design, chiral, engineering, network\n",
      "N - skin, flexible, solar cell, array, nanowire, electronics, three-dimensional, photovoltaics, soft, photonic\n",
      "O - metallic glass, quasicrystals, disorder, transition, amorphous material, icosahedral, negative, silicon, modern, ratio\n",
      "P - colloidal quantum dot, quantum dot solid, nanocrystal, optical gain, triplet excitons, surface science nanocrystals, energy transfer, upconversio...\n",
      "Q - polymer, thermoelectrics, thermoelectric material, thermal conductivity, thermopower, thermal transport, phonon, enhancement, high, clathrate\n",
      "R - water, oxide, proton, surface, anode, direct, h2o2 production, ion, perovskite, electrochemical\n",
      "S - semiconductor, single, nuclear spin, silicon carbide, electron spin, polariton condensate, quantum information, diamond, coherence, manipulation\n",
      "T - topological insulator, photonic, state, perovskite solar cell, excitons, crystalline, crystal, nanoscale, protect, symmetry\n",
      "U - dislocation, strain, x-ray, atomic, electron, three-dimensional imaging, diffraction, coherent, nucleation, microscopy\n",
      "W - electronics, tissue engineering, probe, silicon, neural, biodegradable, vivo, scaffold, imaging, colloidal\n",
      "V - film, polymer glass, nanocomposites, elastic, crystal, thin, droplet, pattern, chaperone, wrinkling\n",
      "X - friction, quantum, dissipation, defect, nanotube, dynamic, carbon, transition, fusion, kitaev\n",
      "Y - high-pressure, high pressure, cubic, synthesis, b2o3, benzene, crn, bulk modulus, tomography, chemistry\n",
      "Z - \n",
      "Saving the network.\n"
     ]
    }
   ],
   "source": [
    "# Some util functions\n",
    "dateoutput = '180423'\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    jsonFileprefix = file[:-5] + '_bardo'\n",
    "    removeStopWords = True;\n",
    "    maximumTokenSize = 3; #n-gram\n",
    "    minKeywordsPerCluster = 10;\n",
    "    maxKeywordsPerCluster = 10;\n",
    "    maxClusterNameLength = 150;\n",
    "    useMajorComponent = True;\n",
    "    verboseMode = True;\n",
    "\n",
    "    network = xn.xnet2igraph(file)\n",
    "    network.vs['wos_id'] = network.vs['name']\n",
    "    network.vs['name'] = network.vs['title']\n",
    "    \n",
    "    \n",
    "    # Obtaining the major connected component (if needed)\n",
    "    if(useMajorComponent):\n",
    "        if(verboseMode): print(\"Obtaining the major connected component.\");\n",
    "        network = network.clusters(\"WEAK\").giant();\n",
    "\n",
    "    # Tokenizing the abstracts\n",
    "    if(verboseMode): print(\"Tokenizing the abstracts.\");\n",
    "    tokensFrequency = [[] for i in range(maximumTokenSize)];\n",
    "    tokensGroupFrequency = [{} for i in range(maximumTokenSize)];\n",
    "\n",
    "    propertiesKeys = set();\n",
    "\n",
    "    verticesTokens = [];\n",
    "    for vertexIndex in range(network.vcount()):\n",
    "        if(vertexIndex%100==0):\n",
    "            print(\"Tokenizing: %d/%d             \"%(vertexIndex,network.vcount()),end=\"\\r\")\n",
    "\n",
    "    #         for wordsList in tokenList:\n",
    "    #             tokens = [set() for i in range(maximumTokenSize)];\n",
    "    #             for wordIndex in range(len(wordsList)):\n",
    "    #                 for tokenSize in range(0,min(wordIndex+1,maximumTokenSize)):\n",
    "    #                     tokens[tokenSize].add(\" \".join(wordsList[(wordIndex-tokenSize):(wordIndex+1)]));\n",
    "        verticesTokens.append(tokenizeString(network.vs[vertexIndex][\"title\"],maximumTokenSize,tokenizerInput));\n",
    "\n",
    "    print(\"Done                   \");\n",
    "\n",
    "    # Obtaining the network community structure\n",
    "    if(verboseMode): print(\"Obtaining the network community structure.\");\n",
    "    \n",
    "\n",
    "    edgelist = [(e.source,e.target) for e in network.es]\n",
    "    communities = infomapApply(network)[0]\n",
    "    communities = [int(c) for c in communities]\n",
    "    print()\n",
    "    \n",
    "    # print(\"Modularity: %f\"%cc.q);\n",
    "\n",
    "    clusters = [[] for i in range(max(communities)+1)];\n",
    "    for vertexIndex in range(network.vcount()):\n",
    "        clusters[communities[vertexIndex]].append(vertexIndex);\n",
    "\n",
    "    #sorting the clusters by size\n",
    "    clusters = sorted(clusters,key=len,reverse=True);\n",
    "\n",
    "    # Getting tokens frequency\n",
    "    if(verboseMode): print(\"Getting tokens frequency.\");\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    tokenFrequencyInClusters = [];\n",
    "    tokenFrequencyInCorpus = {};\n",
    "\n",
    "    for clusterIndex in range(len(clusters)):\n",
    "        cluster = clusters[clusterIndex];\n",
    "        tokenFrequencyInCluster = {};\n",
    "        for vertexIndex in cluster:\n",
    "            tokens = verticesTokens[vertexIndex];\n",
    "            for tokenSize in range(0,maximumTokenSize):\n",
    "                for token in tokens[tokenSize]:\n",
    "                    if(token not in tokenFrequencyInCorpus):\n",
    "                        tokenFrequencyInCorpus[token] = 0;\n",
    "                    if(token not in tokenFrequencyInCluster):\n",
    "                        tokenFrequencyInCluster[token] = 0;\n",
    "                    tokenFrequencyInCorpus[token] += 1;\n",
    "                    tokenFrequencyInCluster[token] += 1;\n",
    "        tokenFrequencyInClusters.append(tokenFrequencyInCluster);\n",
    "\n",
    "    # Calculating the importance Index\n",
    "    if(verboseMode): print(\"Calculating the importance Index.\");\n",
    "    #tokenRelativeFrequencyInClusters = [];\n",
    "    #tokenRelativeFrequencyOutClusters = [];\n",
    "    tokenImportanceIndexInClusters = [];\n",
    "\n",
    "    verticesCount = network.vcount();\n",
    "    for clusterIndex in range(len(clusters)):\n",
    "        clusterSize = len(clusters[clusterIndex]);\n",
    "\n",
    "        tokenFrequencyInCluster = tokenFrequencyInClusters[clusterIndex];\n",
    "\n",
    "        #tokenRelativeFrequencyInCluster = {};\n",
    "        #tokenRelativeFrequencyOutCluster = {};\n",
    "        tokenImportanceIndexInCluster = {};\n",
    "\n",
    "        for token in tokenFrequencyInCluster:\n",
    "            nInCluster = tokenFrequencyInCluster[token];\n",
    "            nOutCluster = tokenFrequencyInCorpus[token]-nInCluster;\n",
    "            outClusterSize = verticesCount-clusterSize;\n",
    "            if(nOutCluster==0):\n",
    "                outClusterSize = 1; #Fix for singletons\n",
    "            FInCluster = float(nInCluster)/float(clusterSize);\n",
    "            FOutCluster = float(nOutCluster)/float(outClusterSize);\n",
    "            importanceIndex = FInCluster-FOutCluster;\n",
    "            #tokenRelativeFrequencyInCluster[token] = FInCluster;\n",
    "            #tokenRelativeFrequencyOutCluster[token] = FOutCluster;\n",
    "            tokenImportanceIndexInCluster[token] = importanceIndex;\n",
    "\n",
    "        #tokenRelativeFrequencyInClusters.append(tokenRelativeFrequencyInCluster);\n",
    "        #tokenRelativeFrequencyOutClusters.append(tokenRelativeFrequencyOutCluster);\n",
    "        tokenImportanceIndexInClusters.append(tokenImportanceIndexInCluster);\n",
    "\n",
    "    defaultNames = \"ABCDEFGHIJKLMNOPQRSTUWVXYZ\";\n",
    "    defaultNamesLength = len(defaultNames);\n",
    "\n",
    "    clusterKeywords = [];\n",
    "    minClusterSize = min([len(cluster) for cluster in clusters]);\n",
    "    maxClusterSize = max([len(cluster) for cluster in clusters]);\n",
    "    clusterNames = [];\n",
    "    for clusterIndex in range(len(clusters)):\n",
    "        cluster = clusters[clusterIndex];\n",
    "        clusterSize = len(cluster);\n",
    "        keywords = [v[0] for v in sorted(tokenImportanceIndexInClusters[clusterIndex].items(),key=operator.itemgetter(1),reverse=True)];\n",
    "        if(maxClusterSize>minClusterSize):\n",
    "            m = (maxKeywordsPerCluster-minKeywordsPerCluster)/float(maxClusterSize-minClusterSize);\n",
    "        else:\n",
    "            m=0;\n",
    "        keywordsCount = round(m*(clusterSize-minClusterSize)+minKeywordsPerCluster);\n",
    "        currentKeywords = [];\n",
    "        while(len(currentKeywords)<keywordsCount and len(keywords)>len(currentKeywords)):\n",
    "            currentKeywords = keywords[0:keywordsCount];\n",
    "            jointKeywords = \".\"+\".\".join(currentKeywords)+\".\";\n",
    "            toRemoveKeywords = [];\n",
    "            for keyword in currentKeywords:\n",
    "                if(jointKeywords.find(\" %s.\"%keyword)>=0):\n",
    "                    toRemoveKeywords.append(keyword);\n",
    "                elif(jointKeywords.find(\".%s \"%keyword)>=0):\n",
    "                    toRemoveKeywords.append(keyword);\n",
    "            for toRemoveKeyword in toRemoveKeywords:\n",
    "                keywords.remove(toRemoveKeyword);\n",
    "                currentKeywords.remove(toRemoveKeyword);\n",
    "        clusterKeywords.append(currentKeywords);\n",
    "        #print(currentKeywords);\n",
    "        clusterName = \"\";\n",
    "        if(clusterIndex<defaultNamesLength):\n",
    "            clusterName += defaultNames[clusterIndex];\n",
    "        else:\n",
    "            clusterName += \"{%d}\"%(clusterIndex);\n",
    "        clusterName += \" - \"+\", \".join(currentKeywords);\n",
    "        if(len(clusterName)>maxClusterNameLength):\n",
    "            clusterName = clusterName[0:maxClusterNameLength-1]+\"...\";\n",
    "        for vertexIndex in cluster:\n",
    "            network.vs[vertexIndex][\"Cluster Name\"] = clusterName;\n",
    "            network.vs[vertexIndex][\"Cluster Index\"] = clusterIndex;\n",
    "        clusterNames.append(clusterName);\n",
    "        print(clusterName);\n",
    "\n",
    "\n",
    "    # Saving the network\n",
    "    if(verboseMode): print(\"Saving the network.\");\n",
    "    # network.vs[\"kcore\"] = network.coreness()\n",
    "\n",
    "    xn.igraph2xnet(network,fileName=PJ('',\"%s_infomap_%s.xnet\"%(jsonFileprefix, dateoutput)),ignoredNodeAtts=[\"Text\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(communities[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xn.igraph2xnet(network,fileName=PJ('',\"%s_infomap_%s.xnet\"%(jsonFileprefix, '301122')),ignoredNodeAtts=[\"Text\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
